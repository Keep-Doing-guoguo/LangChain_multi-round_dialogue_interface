import logging
from typing import Any, List, Dict

from langchain.memory.chat_memory import BaseChatMemory
from langchain.schema import get_buffer_string, BaseMessage, HumanMessage, AIMessage
from langchain.schema.language_model import BaseLanguageModel
from server.db.repository.message_repository import filter_message
from typing import Any, List, Callable
from langchain.chat_models import ChatOpenAI
import os
os.environ["OPENAI_API_KEY"] = ""

# æ³¨æ„æµ‹è¯•çš„æ—¶å€™ï¼Œéœ€è¦å°†filter_messageå‡½æ•°æ‰“å¼€ï¼Œå°†ä¸Šé¢çš„filter_messageå¯¼å…¥è¿›è¡Œå…³é—­ã€‚
# def filter_message(conversation_id: str, limit: int):
#     """æ¨¡æ‹Ÿä»æ•°æ®åº“è¯»å–æœ€è¿‘ N æ¡å¯¹è¯"""
#     print(f"[DB] è¯»å– conversation_id={conversation_id} çš„æœ€è¿‘ {limit} æ¡æ¶ˆæ¯...")
#     return [
#         {"query": "ä½ å¥½", "response": "ä½ å¥½å‘€ï¼"},
#         {"query": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·", "response": "ä»Šå¤©æ™´ï¼Œæ°”æ¸©25åº¦ã€‚"},
#         {"query": "å¸®æˆ‘å†™ä¸€é¦–è¯—", "response": "æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚"}
#     ]
class ConversationBufferDBMemory(BaseChatMemory):
    conversation_id: str
    human_prefix: str = "Human"
    ai_prefix: str = "Assistant"
    llm: BaseLanguageModel
    memory_key: str = "history"
    max_token_limit: int = 2000
    message_limit: int = 10

    @property#@property æ˜¯ Python çš„ä¸€ä¸ªè£…é¥°å™¨ï¼Œè¡¨ç¤ºå°†ä¸€ä¸ªç±»çš„æ–¹æ³•å®šä¹‰ä¸ºä¸€ä¸ªåªè¯»å±æ€§ã€‚ä½¿ç”¨ @property åï¼Œå¯ä»¥åƒè®¿é—®æ™®é€šå±æ€§ä¸€æ ·è°ƒç”¨æ–¹æ³•ï¼Œè€Œæ— éœ€åŠ æ‹¬å·ã€‚
    def buffer(self) -> List[BaseMessage]:
        """String buffer of memory."""
        # fetch limited messages desc, and return reversed

        messages = filter_message(conversation_id=self.conversation_id, limit=self.message_limit)
        # è¿”å›çš„è®°å½•æŒ‰æ—¶é—´å€’åºï¼Œè½¬ä¸ºæ­£åº
        messages = list(reversed(messages))
        chat_messages: List[BaseMessage] = []
        for message in messages:
            chat_messages.append(HumanMessage(content=message["query"]))
            chat_messages.append(AIMessage(content=message["response"]))

        if not chat_messages:
            return []

        # prune the chat message if it exceeds the max token limit
        curr_buffer_length = self.llm.get_num_tokens(get_buffer_string(chat_messages))
        if curr_buffer_length > self.max_token_limit:
            pruned_memory = []
            while curr_buffer_length > self.max_token_limit and chat_messages:
                pruned_memory.append(chat_messages.pop(0))
                curr_buffer_length = self.llm.get_num_tokens(get_buffer_string(chat_messages))

        return chat_messages

    @property
    def memory_variables(self) -> List[str]:
        """Will always return list of memory variables.

        :meta private:
        """
        return [self.memory_key]

    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Return history buffer."""
        buffer: Any = self.buffer
        if self.return_messages:
            final_buffer: Any = buffer
        else:
            final_buffer = get_buffer_string(
                buffer,
                human_prefix=self.human_prefix,
                ai_prefix=self.ai_prefix,
            )
        return {self.memory_key: final_buffer}

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """Nothing should be saved or changed"""
        pass

    def clear(self) -> None:
        """Nothing to clear, got a memory like a vault."""
        pass

#####æµ‹è¯•éƒ¨åˆ†#####
from typing import Any, List
from langchain.schema import get_buffer_string, BaseMessage, HumanMessage, AIMessage
# ====== å°è£…æ¨¡å‹åŠ è½½å‡½æ•° ======
def get_ChatOpenAI(
        model_name: str,
        temperature: float,
        max_tokens: int = None,
        streaming: bool = True,
        callbacks: List[Callable] = [],
        verbose: bool = True,
        **kwargs: Any,
) -> ChatOpenAI:

    model = ChatOpenAI(
        streaming=streaming,
        verbose=verbose,
        callbacks=callbacks,
        openai_api_key='sk-',
        openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        model_name="qwen-plus",
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )
    return model


# ====== main å‡½æ•° ======
def main():
    model = get_ChatOpenAI(
        model_name="qwen-plus",
        temperature=0.7,
        max_tokens=512,
        callbacks=[],
    )

    memory = ConversationBufferDBMemory(conversation_id="test_001", llm=model)
    messages = memory.buffer

    print("\n=== å¯¹è¯å†å² ===")
    for msg in messages:
        role = "ğŸ§‘ Human" if isinstance(msg, HumanMessage) else "ğŸ¤– Assistant"
        print(f"{role}: {msg.content}")

    print("\n=== æ‹¼æ¥ä¸º prompt ===")
    print(get_buffer_string(messages))


if __name__ == "__main__":
    main()